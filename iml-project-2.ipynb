{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":17999,"sourceType":"datasetVersion","datasetId":9726},{"sourceId":9784759,"sourceType":"datasetVersion","datasetId":5994929}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![](https://upload.wikimedia.org/wikipedia/commons/c/ca/The_Serbian_Alphabet_%28Handwriting%29.svg)","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>0 |</span> ABOUT DATASET</b>\n## Context\nFor recognising handwritten forms, the very first step was to gather data in a considerable amount for training. Which I struggled to collect for weeks.\n\n## Content\nThe dataset contains 26 folders (A-Z) containing handwritten images in size 2828 pixels, each alphabet in the image is centre fitted to 2020 pixel box.\n\nEach image is stored as Gray-level\n\nKernel CSVToImages contains script to convert .CSV file to actual images in .png format in structured folder.\n\nNote: Might contain some noisy image as well\n\n## Acknowledgements\nThe images are taken from NIST(https://www.nist.gov/srd/nist-special-database-19) and NMIST large dataset and few other sources which were then formatted as mentioned above.\n\n## Inspiration\nThe dataset would serve beginners in machine learning for there created a predictive model to recognise handwritten characters.","metadata":{}},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>1 |</span> IMPORT LIBRARIES</b>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, r2_score, classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>2 |</span> LOAD DATASET</b>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset/A_Z Handwritten Data.csv')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>3 |</span> SHOW HANDWRITTEN ALPHABETS</b>","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Get the unique classes present in the dataset\nunique_classes = data['0'].unique()\n\n# Create a grid for displaying images\nn_classes = len(unique_classes)\nrows = (n_classes + 5) // 6  # Calculate number of rows needed\nfig, axes = plt.subplots(rows, 6, figsize=(12, 2 * rows))  # Adjust figsize for better spacing\nfig.suptitle('Sample Images for Each Class', fontsize=16)\n\nfor i, class_label in enumerate(unique_classes):\n    # Select the first sample for each class\n    dd = data[data['0'] == class_label].iloc[0]  # Use iloc[0] to get the first sample\n    x = dd[1:].values.reshape((28, 28))  # Reshape into 28x28\n\n    # Plot the image in the corresponding subplot\n    ax = axes[i // 6, i % 6]  # Row = i // 6, Column = i % 6\n    ax.imshow(x, cmap='binary')\n    ax.set_title(f'Class {class_label}')\n    ax.axis('off')  # Hide axis for cleaner visuals\n\n# Hide any empty subplots\nfor j in range(i + 1, rows * 6):\n    axes[j // 6, j % 6].axis('off')\n\n# Adjust layout to avoid overlap\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)  # Adjust top to fit the suptitle\nplt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count the occurrences of each class\nclass_counts = data['0'].value_counts().sort_index()\n\n# Create a bar plot for class distribution\nplt.figure(figsize=(12, 6))\nsns.barplot(x=class_counts.index, y=class_counts.values, palette='viridis')\nplt.title('Class Distribution', fontsize=16)\nplt.xlabel('Class Label', fontsize=12)\nplt.ylabel('Number of Samples', fontsize=12)\nplt.xticks(rotation=90)  # Rotate x labels for better visibility\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Random Sampling","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Assuming your dataframe is named 'data' (first column is labels)\n# and the total number of columns is 785 (1 label + 784 pixel values)\n\n# Calculate the number of samples needed per class\nsamples_per_class = 20000 // data.iloc[:, 0].nunique()  # Assuming balanced classes\n\n# Select samples for each class\nbalanced_data = (\n    data.groupby(data.columns[0])  # Group by the first column (labels)\n    .apply(lambda x: x.sample(n=samples_per_class, random_state=42))  # Random sampling\n    .reset_index(drop=True)  # Reset index after sampling\n)\n\n# Split the balanced data into X (features) and y (labels)\ny = balanced_data.iloc[:, 0]   # First column as labels\nX = balanced_data.iloc[:, 1:]  # Remaining columns as features\n\nprint(f\"Shape of X: {X.shape}\")\nprint(f\"Shape of y: {y.shape}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check the class distribution after random sampling\nclass_counts = y.value_counts()\nprint(\"Class Distribution After Random Sampling:\")\nprint(class_counts)\n\n# Visualize the class distribution\nplt.figure(figsize=(10, 6))\nclass_counts.plot(kind='bar')\nplt.title(\"Class Distribution After Random Sampling\")\nplt.xlabel(\"Class Labels\")\nplt.ylabel(\"Number of Samples\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to plot sample images after random sampling\ndef plot_sample_images(X, y, num_classes=26, samples_per_class=5):\n    fig, axes = plt.subplots(num_classes, samples_per_class, figsize=(10, num_classes * 2))\n    fig.suptitle('Sample Images After Random Sampling', fontsize=16)\n\n    for class_label in range(num_classes):\n        class_samples = X[y == class_label].sample(samples_per_class, random_state=42)\n        for i, ax in enumerate(axes[class_label]):\n            image = class_samples.iloc[i].values.reshape(28, 28)  # Reshape to 28x28\n            ax.imshow(image, cmap='binary')\n            ax.axis('off')\n            ax.set_title(f'Class {class_label}')\n\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)\n    plt.show()\n\n# Call the function after performing random sampling\nplot_sample_images(X, y)  # X and y should be your balanced random sampled dataset\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**STRATIFIED SAMPLING**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\n# Assuming y is your labels Series\nnum_classes = y.nunique()  # Get the number of unique classes\ntotal_samples = len(y)  # Total number of samples in the dataset\n\n# Set the desired samples per class to 769\nsamples_per_class = 769\n\n# Calculate the total sample size and ensure it does not exceed available samples\nsample_size = min(samples_per_class * num_classes, total_samples)\n\n# Ensure we are not attempting to create a test set that is too large\n# Set test_size as a fraction of the total number of samples, at most 1.0 (100%)\ntest_size_fraction = sample_size / total_samples if total_samples > 0 else 0.1  # Prevent division by zero\n\n# Ensure test_size_fraction is within (0, 1)\nif test_size_fraction >= 1.0:\n    test_size_fraction = 0.9  # Set it to 90% to keep some for training\n\n# Create StratifiedShuffleSplit with the desired fraction\nstrat_split = StratifiedShuffleSplit(n_splits=1, test_size=test_size_fraction, random_state=42)\n\n# Perform stratified sampling to get equal representation for each class\nfor _, sample_index in strat_split.split(X, y):\n    stratified_sample_X = X.iloc[sample_index]\n    stratified_sample_y = y.iloc[sample_index]\n\n# Resulting stratified samples\nX_sample = stratified_sample_X\ny_sample = stratified_sample_y\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the shapes of the resulting samples\nprint(f\"Shape of X_sample: {X_sample.shape}\")\nprint(f\"Shape of y_sample: {y_sample.shape}\")\n\n\nprint(\"Stratified Sample Class Distribution:\\n\", y_sample.value_counts())\n\n\nimport matplotlib.pyplot as plt\n\n# Number of classes and samples to display per class\nnum_classes = y_sample.nunique()  # Unique class labels\nsamples_per_class = 5  # Number of samples to display per class\n\n# Create a figure to plot images\nplt.figure(figsize=(num_classes * 2, samples_per_class * 2))  # Adjust figure size\n\n# Loop through each class and plot images\nfor i, class_label in enumerate(y_sample.unique()):\n    # Get samples for the current class\n    class_samples = X_sample[y_sample == class_label].head(samples_per_class)\n\n    for j in range(samples_per_class):\n        plt.subplot(samples_per_class, num_classes, j * num_classes + i + 1)  # Create subplot\n        plt.imshow(class_samples.iloc[j].values.reshape(28, 28), cmap='gray')  # Reshape according to image size\n        plt.axis('off')  # Turn off axis\n        if j == 0:\n            plt.title(f'Class: {class_label}')  # Set title for the first row of each class\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>4 |</span> SPLITTING DATA</b>","metadata":{}},{"cell_type":"code","source":"X = data.drop('0',axis=1)\ny = data['0']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"undersampling ","metadata":{}},{"cell_type":"code","source":"from imblearn.under_sampling import RandomUnderSampler\nimport pandas as pd\n\n# Step 1: Under-sampling to balance classes\nundersampler = RandomUnderSampler(sampling_strategy={i: 769 for i in y.value_counts().index}, random_state=42)\nX_resampled, y_resampled = undersampler.fit_resample(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of X_resampled:\", X_resampled.shape)\nprint(\"Shape of y_resampled:\", y_resampled.shape)\nprint(\"Class distribution after undersampling:\\n\", pd.Series(y_resampled).value_counts())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_images(X, y, n_classes, images_per_class=10):\n    plt.figure(figsize=(n_classes * 2, images_per_class * 2))\n    \n    # Loop through each class\n    for label in range(n_classes):\n        class_indices = np.where(y == label)[0]  # Get indices for the current class\n        selected_indices = np.random.choice(class_indices, images_per_class, replace=False)  # Randomly select images\n        for i, idx in enumerate(selected_indices):\n            plt.subplot(images_per_class, n_classes, i * n_classes + label + 1)\n            plt.imshow(X.iloc[idx].values.reshape(28, 28), cmap='gray')  # Assuming each image is 28x28 pixels\n            plt.axis('off')  # Hide axes\n            if i == 0:  # Show class label only for the first image\n                plt.title(f'Class {label}', fontsize=14)\n\n    plt.tight_layout()\n    plt.show()\n\n# Assuming X_resampled and y_resampled are your resampled data\nn_classes = len(np.unique(y_resampled))  # Get number of classes\nplot_images(X_resampled, y_resampled, n_classes, images_per_class=10)  # Plot images for each class\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**DOWN SAMPLING**","metadata":{}},{"cell_type":"code","source":"from skimage.transform import resize\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef downsample_images(X, new_size=(16, 16)):\n    n_samples = X.shape[0]\n    downsampled_images = np.empty((n_samples, new_size[0] * new_size[1]))\n    \n    # Reshape original images from 28x28 to (n_samples, 28, 28)\n    original_images = X.values.reshape(n_samples, 28, 28)  # Assuming original images are 28x28\n    \n    # Resize all images in one go using a list comprehension\n    downsampled_images = np.array([resize(image, new_size, anti_aliasing=True).flatten() for image in original_images])\n    \n    return downsampled_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsample to 16x16\nX_downsampled_16x16 = downsample_images(X_resampled, new_size=(16, 16))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the shapes of the downsampled images\nprint(\"Shape of downsampled images (16x16):\", X_downsampled_16x16.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_downsampled_images(X, y, n_classes, images_per_class=10, title_suffix=\"\"):\n    plt.figure(figsize=(n_classes * 2, images_per_class * 2))\n    \n    for label in range(n_classes):\n        class_indices = np.where(y == label)[0]\n        selected_indices = np.random.choice(class_indices, images_per_class, replace=False)\n        for i, idx in enumerate(selected_indices):\n            plt.subplot(images_per_class, n_classes, i * n_classes + label + 1)\n            plt.imshow(X[idx].reshape(int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1]))), cmap='gray')  # Reshape based on downsampled size\n            plt.axis('off')\n            if i == 0:\n                plt.title(f'Class {label} {title_suffix}', fontsize=14)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot downsampled images at 16x16\nplot_downsampled_images(X_downsampled_16x16, y_resampled, n_classes, images_per_class=5, title_suffix=\"(8x8)\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downsample to 8x8\nX_downsampled_8x8 = downsample_images(X_resampled, new_size=(8, 8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify the shapes of the downsampled images\nprint(\"Shape of downsampled images (8x8):\", X_downsampled_8x8.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_downsampled_images(X, y, n_classes, images_per_class=10, title_suffix=\"\"):\n    plt.figure(figsize=(n_classes * 2, images_per_class * 2))\n    \n    for label in range(n_classes):\n        class_indices = np.where(y == label)[0]\n        selected_indices = np.random.choice(class_indices, images_per_class, replace=False)\n        for i, idx in enumerate(selected_indices):\n            plt.subplot(images_per_class, n_classes, i * n_classes + label + 1)\n            plt.imshow(X[idx].reshape(int(np.sqrt(X.shape[1])), int(np.sqrt(X.shape[1]))), cmap='gray')  # Reshape based on downsampled size\n            plt.axis('off')\n            if i == 0:\n                plt.title(f'Class {label} {title_suffix}', fontsize=14)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot downsampled images at 8x8\nplot_downsampled_images(X_downsampled_8x8, y_resampled, n_classes, images_per_class=5, title_suffix=\"(6x6)\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GAUSSIAN FILTER","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom skimage.filters import gaussian\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Load your dataset\n# Assuming data is in CSV format with the first column as labels and the rest as pixel values\ndata = pd.read_csv('../input/az-handwritten-alphabets-in-csv-format/A_Z Handwritten Data.csv')  # Change this path to where your file is located\nX = data.iloc[:, 1:]  # Pixel values\ny = data.iloc[:, 0]   # Labels\n\n# Step 1: Perform undersampling\nundersampler = RandomUnderSampler(sampling_strategy='auto', random_state=42)\nX_resampled, y_resampled = undersampler.fit_resample(X, y)\n\n# Step 2: Apply Gaussian filter\ndef apply_gaussian_filter(X, sigma=1.0, image_shape=(28, 28)):\n    blurred_images = np.empty((X.shape[0], image_shape[0] * image_shape[1]))\n    for i in range(X.shape[0]):\n        image = X.iloc[i].values.reshape(image_shape)\n        blurred_image = gaussian(image, sigma=sigma)\n        blurred_images[i] = blurred_image.flatten()\n    return pd.DataFrame(blurred_images)\n\n# Apply Gaussian filter to the resampled dataset\nX_blurred = apply_gaussian_filter(X_resampled, sigma=1.0, image_shape=(28, 28))\n\n# Step 3: Plot images for each class label after Gaussian filtering\ndef plot_images_per_class(X, y, n_classes, images_per_class=5, title_suffix=\"(Gaussian Filter Applied)\"):\n    plt.figure(figsize=(15, images_per_class * 3))  # Increase figure size\n    for label in range(n_classes):\n        indices = np.where(y == label)[0][:images_per_class]  # Select images_per_class instances per class\n        for i, idx in enumerate(indices):\n            plt.subplot(images_per_class, n_classes, i * n_classes + label + 1)\n            plt.imshow(X.iloc[idx].values.reshape(28, 28), cmap='gray')\n            plt.axis('off')\n            if i == 0:\n                plt.title(f\"{label}\", fontsize=14)  # Only show class number\n    plt.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust spacing between subplots\n    plt.suptitle(f\"Sample Images per Class {title_suffix}\", fontsize=18)  # Increase overall title font size\n    plt.show()\n\n# Define number of classes and plot images\nn_classes = len(np.unique(y_resampled))  # Number of unique classes in y_resampled\nplot_images_per_class(X_blurred, y_resampled, n_classes, images_per_class=5)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"PCA","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_blurred)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier  # or another classifier of your choice\nfrom sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_blurred)\n\n# Assuming X_scaled is your scaled feature matrix and y_resampled is your target variable\n# Step 1: Split Data into Training and Testing Sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Step 2: Perform PCA\n# Set the number of components based on desired explained variance or fixed number of components\npca = PCA(n_components=0.95)  # Retains 95% variance; adjust as needed\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Step 3: Initialize and Train Classifier\nclassifier = RandomForestClassifier(random_state=42)  # Replace with your classifier if needed\nclassifier.fit(X_train_pca, y_train)\n\n# Step 4: Make Predictions\ny_pred = classifier.predict(X_test_pca)\n\n# Step 5: Evaluate the Model\n# Overall accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Overall Accuracy:\", accuracy)\n\n# F1 Score (weighted and macro)\nf1_weighted = f1_score(y_test, y_pred, average='weighted')\nf1_macro = f1_score(y_test, y_pred, average='macro')\nprint(\"Weighted F1 Score:\", f1_weighted)\nprint(\"Macro F1 Score:\", f1_macro)\n\n# Classification Report (detailed with macro and weighted averages)\nreport = classification_report(y_test, y_pred, zero_division=0, output_dict=True)\n\n# Extract and display weighted and macro avg from the classification report\nweighted_avg = report['weighted avg']\nmacro_avg = report['macro avg']\n\nprint(\"\\nDetailed Classification Report:\")\nprint(\"Weighted Avg - F1 Score:\", weighted_avg['f1-score'])\nprint(\"Weighted Avg - Precision:\", weighted_avg['precision'])\nprint(\"Weighted Avg - Recall:\", weighted_avg['recall'])\nprint(\"Macro Avg - F1 Score:\", macro_avg['f1-score'])\nprint(\"Macro Avg - Precision:\", macro_avg['precision'])\nprint(\"Macro Avg - Recall:\", macro_avg['recall'])\n\n# Display the full classification report in a readable format\nprint(\"\\nFull Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(\"\\nConfusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"BAGGING CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Initialize the base estimator (e.g., Decision Tree) for Bagging\ndt_classifier = DecisionTreeClassifier(random_state=42)\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Initialize and train the Bagging Classifier with the base estimator\nbagging_classifier = BaggingClassifier(estimator=dt_classifier, n_estimators=10, max_samples=0.8, max_features=0.8, random_state=42)\n\n# Fit the Bagging Classifier using the training data\nbagging_classifier.fit(X_train, y_train)\n\n# Make predictions and evaluate the Bagging Classifier\ny_pred_bagging = bagging_classifier.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_bagging)\nreport = classification_report(y_test, y_pred_bagging)\nconf_matrix = confusion_matrix(y_test, y_pred_bagging)\n\n# Print metrics\nprint(\"Bagging Classifier Accuracy:\", accuracy)\nprint(\"Bagging Classifier Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"VOTING CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Define individual classifiers for Voting\ndt_classifier = DecisionTreeClassifier(random_state=42)\nrf_classifier = RandomForestClassifier(n_estimators=50, random_state=42)\nnb_classifier = GaussianNB()\n\n# Step 2: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Initialize and train the Voting Classifier with the selected classifiers\nvoting_classifier = VotingClassifier(\n    estimators=[('dt', dt_classifier), ('rf', rf_classifier), ('nb', nb_classifier)],\n    voting='hard'  # Use 'soft' if classifiers support probability outputs\n)\n\n# Fit the Voting Classifier using the training data\nvoting_classifier.fit(X_train, y_train)\n\n# Make predictions and evaluate the Voting Classifier\ny_pred_voting = voting_classifier.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_voting)\nreport = classification_report(y_test, y_pred_voting)\nconf_matrix = confusion_matrix(y_test, y_pred_voting)\n\n# Print metrics\nprint(\"Voting Classifier Accuracy:\", accuracy)\nprint(\"Voting Classifier Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"NAIVE BAYES CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Split the data into training and testing sets (ensure you have X_scaled and y_resampled defined)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Initialize and train the Naive Bayes Classifier\nnb_classifier = GaussianNB()\nnb_classifier.fit(X_train, y_train)  # Use the training data\n\n# Make predictions and evaluate the Naive Bayes Classifier\ny_pred_nb = nb_classifier.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_nb)\nreport = classification_report(y_test, y_pred_nb)\nconf_matrix = confusion_matrix(y_test, y_pred_nb)\n\n# Print metrics\nprint(\"Naive Bayes Classifier Accuracy:\", accuracy)\nprint(\"Naive Bayes Classifier Report:\\n\", report)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SVM","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# Assuming you have your data scaled and prepared as X_scaled and y_resampled\n# Step 1: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Initialize and train the Support Vector Machine Classifier\nsvm_classifier = SVC(random_state=42)  # You can adjust parameters as needed\nsvm_classifier.fit(X_train, y_train)  # Use the training data\n\n# Make predictions and evaluate the SVM Classifier\ny_pred_svm = svm_classifier.predict(X_test)\n\n# Evaluate metrics\naccuracy = accuracy_score(y_test, y_pred_svm)\nconfusion = confusion_matrix(y_test, y_pred_svm)\nprecision = precision_score(y_test, y_pred_svm, average='weighted')\nrecall = recall_score(y_test, y_pred_svm, average='weighted')\nf1 = f1_score(y_test, y_pred_svm, average='weighted')\n\n# Print results\nprint(\"SVM Classifier Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", confusion)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"SVM Classifier Report:\\n\", classification_report(y_test, y_pred_svm))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"KNN","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\n\n# Assuming you have your data scaled and prepared as X_scaled and y_resampled\n# Step 1: Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)\n\n# Initialize and train the k-Nearest Neighbors Classifier\nknn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\nknn_classifier.fit(X_train, y_train)  # Use the training data\n\n# Make predictions and evaluate the KNN Classifier\ny_pred_knn = knn_classifier.predict(X_test)\n\n# Evaluate metrics\naccuracy = accuracy_score(y_test, y_pred_knn)\nconfusion = confusion_matrix(y_test, y_pred_knn)\nprecision = precision_score(y_test, y_pred_knn, average='weighted')\nrecall = recall_score(y_test, y_pred_knn, average='weighted')\nf1 = f1_score(y_test, y_pred_knn, average='weighted')\n\n# Print results\nprint(\"KNN Classifier Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", confusion)\nprint(\"Precision:\", precision)\nprint(\"Recall:\", recall)\nprint(\"F1 Score:\", f1)\nprint(\"KNN Classifier Report:\\n\", classification_report(y_test, y_pred_knn))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RANDOM FOREST CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\n\n# Step 1: Initialize the Random Forest Classifier\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Step 2: Train the model on the training data\nrf_model.fit(X_train, y_train)\n\n# Step 3: Make predictions on the test data\ny_pred_rf = rf_model.predict(X_test)\n\n# Step 4: Evaluate the model\naccuracy = accuracy_score(y_test, y_pred_rf)\nprecision = precision_score(y_test, y_pred_rf, average='weighted')  # average='weighted' considers class imbalance\nrecall = recall_score(y_test, y_pred_rf, average='weighted')\nf1 = f1_score(y_test, y_pred_rf, average='weighted')\n\nprint(\"Random Forest Classifier Accuracy:\", accuracy)\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\nprint(\"\\nOverall Precision:\", precision)\nprint(\"Overall Recall:\", recall)\nprint(\"Overall F1-Score:\", f1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"DTC CLASSIFIER ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n\n# Step 7: Initialize and train the Decision Tree Classifier\ndtc_model = DecisionTreeClassifier(random_state=42)\ndtc_model.fit(X_train, y_train)\n\n# Step 8: Make predictions and evaluate the model\ny_pred_dtc = dtc_model.predict(X_test)\n\n# Calculate and display accuracy\naccuracy_dtc = accuracy_score(y_test, y_pred_dtc)\nprint(\"Decision Tree Classifier Accuracy:\", accuracy_dtc)\n\n# Display classification report\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_dtc))\n\n# Calculate overall precision, recall, and F1-score\nprecision_dtc = precision_score(y_test, y_pred_dtc, average='weighted')\nrecall_dtc = recall_score(y_test, y_pred_dtc, average='weighted')\nf1_dtc = f1_score(y_test, y_pred_dtc, average='weighted')\n\nprint(\"\\nOverall Precision:\", precision_dtc)\nprint(\"Overall Recall:\", recall_dtc)\nprint(\"Overall F1-Score:\", f1_dtc)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(classification_report(y_test,ypred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b><span style='color:#F1C40F'>7 |</span> SHOW PREDICTED LABEL</b>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/dataset/A_Z Handwritten Data.csv')\n\n# Assuming the dataset contains a label column named 'label'\nlabels = data['label']\nX = data.drop(columns=['label'])\n\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n\n# Scale features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Train SVM model\nsvm_model = svm.SVC(kernel='linear')\nsvm_model.fit(X_train_scaled, y_train)\n\n# Function to predict and display a handwritten character\ndef perdhandwritten(ind):\n    x = X_test.iloc[ind].values.reshape(1, -1)\n    row_scaled = scaler.transform(x)\n    y_pred = svm_model.predict(row_scaled)\n    \n    plot_data = X_test.iloc[ind].values.reshape(28, 28)\n    \n    plt.figure(dpi=150)\n    plt.title(f'Model Prediction: {chr(y_pred[0] + 65)}')  # Convert label to alphabet\n    plt.imshow(plot_data, cmap='gray')\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nperdhandwritten(10)  # Example index\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"perdhandwritten(3323)\nperdhandwritten(15323)\nperdhandwritten(25323)\nperdhandwritten(38323)\nperdhandwritten(48323)\nperdhandwritten(58323)\nperdhandwritten(68323)\nperdhandwritten(78323)\nperdhandwritten(88323)\nperdhandwritten(98323)\nperdhandwritten(108323)\nperdhandwritten(118323)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The End...","metadata":{}}]}